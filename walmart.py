# -*- coding: utf-8 -*-
"""WALMART.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16G3qKGuYVgldTJ_iGkYs5vUbAvb7m72n
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

df=pd.read_csv('/content/Walmart.csv')

df

df.info()

df['Date'] = pd.to_datetime(df['Date'], format='%d-%m-%Y')
df['Year'] = df['Date'].dt.year
df['Month'] = df['Date'].dt.month

df.head()

df.info()

df.isnull().sum()

df.duplicated().sum()

plt.figure(figsize=(12, 6))
sns.boxplot(y=df['Weekly_Sales'])
plt.title('Boxplot of Weekly Sales (Before Outlier Removal)')
plt.show()

# Select numeric columns only (excluding 'Store' and 'Holiday_Flag' if you want to keep them)
numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns
# Optionally exclude columns you do not want to check
numeric_cols = [col for col in numeric_cols if col not in ['Store', 'Holiday_Flag']]

# Check and remove outliers for each numeric column
for col in numeric_cols:
   # Plot before removing
    plt.figure(figsize=(10, 5))
    sns.boxplot(y=df[col])
    plt.title(f'Boxplot of {col} (Before Outlier Removal)')
    plt.show()

# Calculate IQR
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Remove outliers for this column
    df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]

    # Plot after removing
    plt.figure(figsize=(10, 5))
    sns.boxplot(y=df[col])
    plt.title(f'Boxplot of {col} (After Outlier Removal)')
    plt.show()

df.info()

df.head()

"""✅ a. Are weekly sales affected by unemployment rate? Which stores suffer the most?"""

# Overall correlation
corr_unemployment = df['Weekly_Sales'].corr(df['Unemployment'])
print(f'Overall correlation between Weekly Sales and Unemployment: {corr_unemployment:.2f}')

# Store-wise correlation
store_corr = df.groupby('Store')[['Weekly_Sales', 'Unemployment']].corr().iloc[0::2, -1].reset_index()
store_corr = store_corr[['Store', 'Unemployment']]
store_corr.columns = ['Store', 'Unemployment_Corr']

# Stores most negatively affected (lowest correlations)
print('Stores most negatively affected by Unemployment:')
print(store_corr.sort_values(by='Unemployment_Corr').head())

"""✅ b. Do weekly sales show seasonal trends? When and why?"""

import seaborn as sns
import matplotlib.pyplot as plt

# Add month column if not already added
df['Month'] = df['Date'].dt.month

# Plot average monthly sales
plt.figure(figsize=(10, 5))
sns.lineplot(x='Month', y='Weekly_Sales', data=df, estimator='mean', ci=None)
plt.title('Average Monthly Weekly Sales (Seasonal Trend)')
plt.xlabel('Month')
plt.ylabel('Average Weekly Sales')
plt.xticks(range(1, 13))
plt.show()

"""✅ c. Does temperature affect weekly sales?"""

# Overall correlation
corr_temp = df['Weekly_Sales'].corr(df['Temperature'])
print(f'Correlation between Weekly Sales and Temperature: {corr_temp:.2f}')

# Scatter plot
plt.figure(figsize=(8, 5))
sns.scatterplot(x='Temperature', y='Weekly_Sales', data=df, alpha=0.3)
plt.title('Weekly Sales vs Temperature')
plt.show()

"""✅ d. How is CPI affecting weekly sales in various stores?"""

# Overall correlation
corr_cpi = df['Weekly_Sales'].corr(df['CPI'])
print(f'Overall correlation between Weekly Sales and CPI: {corr_cpi:.2f}')

# Store-wise correlation
store_corr_cpi = df.groupby('Store')[['Weekly_Sales', 'CPI']].corr().iloc[0::2, -1].reset_index()
store_corr_cpi = store_corr_cpi[['Store', 'CPI']]
store_corr_cpi.columns = ['Store', 'CPI_Corr']

print('Store-wise correlation with CPI (first 5 stores):')
print(store_corr_cpi.sort_values(by='CPI_Corr').head())

"""✅ e. Top performing stores (by total historical sales)"""

store_sales = df.groupby('Store')['Weekly_Sales'].sum().sort_values(ascending=False)
print('Top 5 performing stores:')
print(store_sales.head(5))

"""✅ f. Worst performing store and difference with the best"""

worst_store = store_sales.tail(1)
best_store_sales = store_sales.max()
worst_store_sales = store_sales.min()
difference = best_store_sales - worst_store_sales

print('Worst performing store:')
print(worst_store)
print(f'Difference between highest and lowest store sales: {difference:.2f}')

""" Check stationarity (ADF test)"""

df

from statsmodels.tsa.seasonal import STL

# Load your dataset
df = pd.read_csv('Walmart.csv')
df['Date'] = pd.to_datetime(df['Date'], format='%d-%m-%Y')

# Example: Choose one store (e.g., Store 1)
store_id = 1
store_df = df[df['Store'] == store_id].sort_values('Date')

# Set Date as index
store_df = store_df.set_index('Date')

# Create a complete date range to ensure no missing weeks
full_date_range = pd.date_range(start=store_df.index.min(),
                               end=store_df.index.max(),
                               freq='W')
store_df = store_df.reindex(full_date_range)

# Interpolate missing values (only for Weekly_Sales)
store_df['Weekly_Sales'] = store_df['Weekly_Sales'].interpolate(method='time')

# Fill other columns with forward fill (or choose appropriate method)
for col in store_df.columns:
    if col != 'Weekly_Sales':
        store_df[col] = store_df[col].ffill()

# Perform STL decomposition
try:
    stl = STL(store_df['Weekly_Sales'], period=52)  # 52 weeks in a year
    result = stl.fit()

    # Plot
    plt.figure(figsize=(12, 8))
    result.plot()
    plt.suptitle(f'Store {store_id} - STL Decomposition', fontsize=16)
    plt.tight_layout()
    plt.show()

except Exception as e:
    print(f"Error during STL decomposition: {e}")

store_df = df[df['Store'] == 1].sort_values('Date').set_index('Date')

# Plot sales over time
plt.figure(figsize=(12, 6))
plt.plot(store_df.index, store_df['Weekly_Sales'])
plt.title('Store 1 - Weekly Sales Over Time')
plt.xlabel('Date')
plt.ylabel('Weekly Sales')
plt.grid()
plt.show()

# Calculate rolling mean and std
rolling_mean = store_df['Weekly_Sales'].rolling(window=12).mean()
rolling_std = store_df['Weekly_Sales'].rolling(window=12).std()

# Plot rolling statistics
plt.figure(figsize=(12, 6))
plt.plot(store_df['Weekly_Sales'], label='Original')
plt.plot(rolling_mean, label='Rolling Mean', color='red')
plt.plot(rolling_std, label='Rolling Std', color='black')
plt.legend()
plt.title('Rolling Mean & Standard Deviation')
plt.show()

from statsmodels.tsa.stattools import adfuller

# Perform ADF test
adf_result = adfuller(store_df['Weekly_Sales'], autolag='AIC')

# Print results
print('ADF Statistic:', adf_result[0])
print('p-value:', adf_result[1])
print('Critical Values:')
for key, value in adf_result[4].items():
    print(f'\t{key}: {value}')

# Interpret results
if adf_result[1] < 0.05:
    print("✅ Reject Null Hypothesis (Data is Stationary)")
else:
    print("❌ Fail to Reject Null Hypothesis (Data is Non-Stationary)")

from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.seasonal import seasonal_decompose
from sklearn.metrics import mean_absolute_error, mean_squared_error
import warnings
warnings.filterwarnings('ignore')

train = store_df.iloc[:-12]  # All except last 12 weeks
test = store_df.iloc[-12:]   # Last 12 weeks for testing

# SARIMA(p,d,q)(P,D,Q,s)
model = SARIMAX(train['Weekly_Sales'],
                order=(1, 1, 1),           # Non-seasonal (AR=1, I=1, MA=1)
                seasonal_order=(1, 1, 1, 52),  # Seasonal (yearly)
                enforce_stationarity=False)
results = model.fit(disp=False)
print(results.summary())

# Generate predictions
forecast = results.get_forecast(steps=12)
forecast_mean = forecast.predicted_mean
conf_int = forecast.conf_int()  # 95% confidence interval

# Ensure test data and forecast have matching indices
try:
    # Align indices explicitly
    test_aligned = test['Weekly_Sales'].reindex(forecast_mean.index)

    # Calculate errors only where both test and forecast exist
    valid_mask = ~test_aligned.isna() & ~forecast_mean.isna()

    if valid_mask.any():
        mae = mean_absolute_error(test_aligned[valid_mask], forecast_mean[valid_mask])
        rmse = np.sqrt(mean_squared_error(test_aligned[valid_mask], forecast_mean[valid_mask]))
        print(f"MAE: {mae:.2f}, RMSE: {rmse:.2f}")
    else:
        print("Error: No overlapping data points for evaluation")

except Exception as e:
    print(f"Evaluation error: {str(e)}")
    print("Debugging info:")
    print(f"Test dates: {test.index}")
    print(f"Forecast dates: {forecast_mean.index}")

plt.figure(figsize=(12, 6))
plt.plot(train.index, train['Weekly_Sales'], label='Training Data')
plt.plot(test.index, test['Weekly_Sales'], label='Actual Sales', color='green')
plt.plot(forecast_mean.index, forecast_mean, label='Forecast', color='red')
plt.fill_between(conf_int.index, conf_int.iloc[:, 0], conf_int.iloc[:, 1], color='pink', alpha=0.3)
plt.title('Walmart Store 1 - 12-Week Sales Forecast')
plt.xlabel('Date')
plt.ylabel('Weekly Sales')
plt.legend()
plt.grid()
plt.show()

stores = [1, 2, 3, 4, 5]
forecasts = {}

for store in stores:
    store_data = df[df['Store'] == store].set_index('Date').asfreq('W')
    store_data['Weekly_Sales'] = store_data['Weekly_Sales'].interpolate()

    model = SARIMAX(store_data['Weekly_Sales'],
                    order=(1, 1, 1),
                    seasonal_order=(1, 1, 1, 52))
    results = model.fit(disp=False)

    forecast = results.get_forecast(steps=12)
    forecasts[store] = {
        'Forecast': forecast.predicted_mean,
        'Lower_CI': forecast.conf_int().iloc[:, 0],
        'Upper_CI': forecast.conf_int().iloc[:, 1]
    }

# Display forecasts
for store, data in forecasts.items():
    print(f"\nStore {store} Forecast (Next 12 Weeks):")
    print(pd.DataFrame({
        'Date': data['Forecast'].index,
        'Forecasted_Sales': data['Forecast'].values,
        'Lower_Bound': data['Lower_CI'].values,
        'Upper_Bound': data['Upper_CI'].values
    }))

